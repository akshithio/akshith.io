---
title: "A top level understanding of vector embeddings"
url: "a-top-level-understanding-of-vector-embeddings"
date: "2020-01-01"
category: "technical"
views: "24,384"
---

Natural Language Processing (NLP) tasks rely on the ability for computers to understand
human language and the true meanings associated with each word. Algorithms that are able
to capture the nuance of these words via numeric representations are referred to as word
embedding algorithms.

Image 1: A diagram describing vectors and stuff [creds]

Alternatively, while a significant correlation score improvement is observed with a nearly
1000-fold increase in dataset size [@1], the gains are not linearly proportional but exhibit
a more logarithmic trend. This suggests diminishing returns as dataset size grows, indicating
that fastText optimally utilises its available data, rather than GloVe disproportionately
benefiting from larger dataset sizes.

This is some **bold** and _italics_ text.

This is a list in markdown:

- One
- Two
- Three

Checkout my React component:
